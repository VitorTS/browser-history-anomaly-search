{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use\n",
    "\n",
    "1. install export chrome history:\n",
    "https://chrome.google.com/webstore/detail/export-chrome-history/dihloblpkeiddiaojbagoecedbfpifdj?hl=en\n",
    "\n",
    "2. export one week worth of history as csv:\n",
    "\n",
    "![alt text](print.png \"Title\")\n",
    "\n",
    "3. press Ctrl-F9 to run all cells o this notebook (will take a while)\n",
    "\n",
    "4. Upload the csv file:\n",
    "\n",
    "![alt text](print2.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#interactive stuff\n",
    "import io\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import interactive, FileUpload, Output\n",
    "\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def generate_embeddings(messages_in):\n",
    "     return embed(messages_in).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data(df):\n",
    "    df['text'] = df['title']\n",
    "    \n",
    "    df.dropna(\n",
    "        axis=0,\n",
    "        how='any',\n",
    "        thresh=None,\n",
    "        subset=['text'],\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    df['index'] = df.index\n",
    "    \n",
    "    #clear non meaningful words\n",
    "    import re\n",
    "\n",
    "    noiseWords = ['Google Search', '|', '%', '.', ' â€” ', '/']\n",
    "    big_regex = re.compile('|'.join(map(re.escape, noiseWords)))\n",
    "    df['text'] = df['text'].apply(lambda x : big_regex.sub(\"\", x) )\n",
    "\n",
    "    def remove_non_nouns(lines):\n",
    "        is_noun = lambda pos: pos[:2] == 'NN'\n",
    "        tokenized = nltk.word_tokenize(lines)\n",
    "        return ' '.join( [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] )\n",
    "\n",
    "    df['text'] = df['text'].apply(remove_non_nouns);\n",
    "    \n",
    "    #remove duplicates, NaNs etc\n",
    "    df.drop_duplicates(subset=['text'],inplace=True)\n",
    "\n",
    "    df.dropna(\n",
    "        axis=0,\n",
    "        how='any',\n",
    "        thresh=None,\n",
    "        subset=['text'],\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    df['index'] = df.index\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_most_common_sentence(df):\n",
    "    #get word frequency\n",
    "    tokens = nltk.tokenize.word_tokenize(' '.join(df[\"text\"]))\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    \n",
    "    compare_to_top = 10\n",
    "    top_n_words_sentence = ' '.join([i[0] for i in freq.most_common(compare_to_top)])\n",
    "    return df.append({'text': top_n_words_sentence, 'type': 1}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_narest_neighbor(df, use_embeddings):\n",
    "    #calculat the distance between USE embeddings\n",
    "    nn_tree = AnnoyIndex(512, 'euclidean')\n",
    "    for idx, e in enumerate(use_embeddings):\n",
    "        nn_tree.add_item(idx, e)\n",
    "    nn_tree.build(10)\n",
    "    \n",
    "    idxs = nn_tree.get_nns_by_item(df.last_valid_index(), len(df))\n",
    "    \n",
    "    idxs.reverse() #get furthest neighhbors\n",
    "    \n",
    "    return df.reindex(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d3d925ac934d63855744c1a8e0a2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='70px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922cb98d3fa4a3786eab48188c8e492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='400px', overflow='scroll'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outFileUpload = Output(layout={'border': '1px solid black', 'height': '70px'})\n",
    "out = Output(layout={'border': '1px solid black', 'height': '400px', 'overflow': 'scroll'})\n",
    "\n",
    "uploader = FileUpload(\n",
    "    accept='.csv,.txt',\n",
    "    multiple=False\n",
    ")\n",
    "\n",
    "with outFileUpload:\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    display(uploader)\n",
    "\n",
    "@out.capture()\n",
    "def on_upload_change(change):\n",
    "    df = pd.read_csv(io.BytesIO(change['owner'].data[0]))\n",
    "\n",
    "    out.clear_output(wait=True)\n",
    "    \n",
    "    if not 'title' in df:\n",
    "        return display(\"error: csv file needs to have a 'title' column\")\n",
    "    \n",
    "    df = clear_data(df)\n",
    "    df = append_most_common_sentence(df)\n",
    "    \n",
    "    compSentence = df.tail(1)\n",
    "    \n",
    "    use_embeddings = generate_embeddings(df['text'])\n",
    "    \n",
    "    df = sort_by_narest_neighbor(df, use_embeddings)\n",
    "    \n",
    "    df.drop(compSentence.index, inplace=True)\n",
    "    \n",
    "    display(\"comparing sentences to '\" + compSentence['text'].values[0] + \"''\")\n",
    "    \n",
    "    display(df[['date', 'time', 'title']])\n",
    "        \n",
    "uploader.observe(on_upload_change, names='_counter')\n",
    "\n",
    "display(outFileUpload)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
