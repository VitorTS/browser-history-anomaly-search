{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use\n",
    "\n",
    "1. install export chrome history:\n",
    "https://chrome.google.com/webstore/detail/export-chrome-history/dihloblpkeiddiaojbagoecedbfpifdj?hl=en\n",
    "\n",
    "2. export one week worth of history as csv\n",
    "![alt text](print.png \"Title\")\n",
    "\n",
    "3. press Ctrl-f9 to run all cells o this notebook\n",
    "\n",
    "4. Upload the csv file\n",
    "![alt text](print2.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\work\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "C:\\Users\\work\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_sentencepiece\\_sentencepiece_processor_ops.so not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-286ec1455e3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtf_sentencepiece\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_sentencepiece\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtf_sentencepiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentencepiece_processor_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_sentencepiece\\sentencepiece_processor_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m _gen_sentencepiece_processor_op = tf.load_op_library(\n\u001b[0m\u001b[0;32m     27\u001b[0m     os.path.join(os.path.dirname(__file__), '_sentencepiece_processor_ops.so'))\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\load_library.py\u001b[0m in \u001b[0;36mload_op_library\u001b[1;34m(library_filename)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0munable\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpython\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m   \"\"\"\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[0mlib_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpy_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_LoadLibrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibrary_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     wrappers = _pywrap_python_op_gen.GetPythonWrappers(\n",
      "\u001b[1;31mNotFoundError\u001b[0m: C:\\Users\\work\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_sentencepiece\\_sentencepiece_processor_ops.so not found"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "import tensorflow_hub as hub\n",
    "import tf_sentencepiece\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#interactive stuff\n",
    "import io\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import interactive, FileUpload, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU, ah :(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #teste if gpu is enabled\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "        c = tf.matmul(a, b)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print (sess.run(c))\n",
    "    # should be\n",
    "    #   [[22. 28.]\n",
    "    #   [49. 64.]]\n",
    "except:\n",
    "    print(\"no GPU, ah :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Universal Sentence encoder (USE)]\n",
    "%%capture\n",
    "use_module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/1\"\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed_module = hub.Module(use_module_url)\n",
    "    embedded_text = embed_module(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "config = tf.ConfigProto(inter_op_parallelism_threads=1,\n",
    "                   intra_op_parallelism_threads=1)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "session = tf.Session(graph=g, config=config)\n",
    "\n",
    "session.run(init_op)\n",
    "\n",
    "\n",
    "\n",
    "def generate_embeddings(messages_in):\n",
    "    return session.run(embedded_text, feed_dict={text_input: messages_in})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data(df):\n",
    "    #remove duplicates, NaNs etc\n",
    "    df.drop_duplicates(subset=['title'],inplace=True)\n",
    "\n",
    "    df.dropna(\n",
    "        axis=0,\n",
    "        how='any',\n",
    "        thresh=None,\n",
    "        subset=['title'],\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    df['index'] = df.index\n",
    "    \n",
    "    #clear non meaningful words\n",
    "    import re\n",
    "\n",
    "    noiseWords = ['Google Search', '|', '%', '.', ' â€” ', '/']\n",
    "    big_regex = re.compile('|'.join(map(re.escape, noiseWords)))\n",
    "    df['title'] = df['title'].apply(lambda x : big_regex.sub(\"\", x) )\n",
    "\n",
    "    def remove_non_nouns(lines):\n",
    "        is_noun = lambda pos: pos[:2] == 'NN'\n",
    "        tokenized = nltk.word_tokenize(lines)\n",
    "        return ' '.join( [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] )\n",
    "\n",
    "    df['title'] = df['title'].apply(remove_non_nouns);\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_most_common_sentence(df):\n",
    "    #get word frequency\n",
    "    tokens = nltk.tokenize.word_tokenize(' '.join(df[\"title\"]))\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    \n",
    "    compare_to_top = 10\n",
    "    top_n_words_sentence = ' '.join([i[0] for i in freq.most_common(compare_to_top)])\n",
    "    return df.append({'title': top_n_words_sentence, 'type': 1}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_narest_neighbor(df, use_embeddings):\n",
    "    #return df\n",
    "    #calculat the distance between USE embeddings\n",
    "    from annoy import AnnoyIndex\n",
    "    nn_tree = AnnoyIndex(512, 'euclidean')\n",
    "    for idx, e in enumerate(use_embeddings):\n",
    "        nn_tree.add_item(idx, e)\n",
    "    nn_tree.build(10)\n",
    "    \n",
    "    idxs = nn_tree.get_nns_by_item(df.last_valid_index(), len(df))\n",
    "    \n",
    "    idxs.reverse() #get furthest neighhbors\n",
    "    \n",
    "    return df.reindex(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d3d925ac934d63855744c1a8e0a2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='70px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922cb98d3fa4a3786eab48188c8e492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='400px', overflow='scroll'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outFileUpload = Output(layout={'border': '1px solid black', 'height': '70px'})\n",
    "out = Output(layout={'border': '1px solid black', 'height': '400px', 'overflow': 'scroll'})\n",
    "\n",
    "uploader = FileUpload(\n",
    "    accept='.csv,.txt',\n",
    "    multiple=False\n",
    ")\n",
    "\n",
    "with outFileUpload:\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    display(uploader)\n",
    "\n",
    "@out.capture()\n",
    "def on_upload_change(change):\n",
    "    df = pd.read_csv(io.BytesIO(change['owner'].data[0]))\n",
    "\n",
    "    out.clear_output(wait=True)\n",
    "    \n",
    "    if not 'title' in df:\n",
    "        return display(\"error: csv file needs to have a 'title' column\")\n",
    "    \n",
    "    df = clear_data(df)\n",
    "    df = append_most_common_sentence(df)\n",
    "    \n",
    "    compSentence = df.tail(1)\n",
    "    \n",
    "    use_embeddings = generate_embeddings(df['title'])\n",
    "    #use_embeddings = np.load('embeddings.npy')\n",
    "    df = sort_by_narest_neighbor(df, use_embeddings)\n",
    "    \n",
    "    df.drop(compSentence.index, inplace=True)\n",
    "    \n",
    "    display(\"comparing sentences to '\" + compSentence['title'].values[0] + \"''\")\n",
    "    \n",
    "    display(df[['date', 'time', 'title']])\n",
    "        \n",
    "uploader.observe(on_upload_change, names='_counter')\n",
    "\n",
    "display(outFileUpload)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
